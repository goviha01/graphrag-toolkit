{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b95df03",
   "metadata": {},
   "source": [
    "# 2a - Metadata Filtering\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete <a href=\"../../../../nbclassic/notebooks/graphrag-toolkit/2-Querying.ipynb\"><b>Exercise 2 - Querying</b></a> before beginning these additional exercises.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Metadata filtering allows for constrained retrieval of information based on specific criteria. It can be applied at various stages of the process, including document extraction, graph building, and querying.\n",
    "\n",
    "When querying a lexical graph, metadata filtering enables the retrieval of a constrained set of statements, sources, and topics based on specified metadata filters and associated values. This functionality is particularly useful for narrowing down search results to relevant information.\n",
    "\n",
    "The GraphRAG Toolkit utilizes LlamaIndex types such as `MetadataFilters`, `MetadataFilter`, `FilterOperator`, and `FilterCondition` to specify filter criteria. These components allow you to create complex and nested filter expressions.\n",
    "\n",
    "### Metadata filtering versus multi-tenancy\n",
    "\n",
    "Metadata filtering and multi-tenancy work well together. Multi tenancy restricts access to one of many _wholly separate_ lexical graphs within the same underlying graph and vector stores. Metadata filtering constrains retrieval to one or more _subgraphs_ within a particular lexical graph:\n",
    "\n",
    "![Metadata Filtering](../images/metadata-filtering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa1ad2",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ 2a.1 Query the data using graph-enhanced search with metadata filtering\n",
    "\n",
    "The following query applies a metadata filter that restricts the data to source documents whose `pub_date` is before 1st September 2025. Because the news of the blockage in the Turquoise Canal was published after 1st September, the answer returned by the query does not take account of this unfortunate development, and is thus more optimistic about Example Corp's fortunes in the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.metadata import FilterConfig\n",
    "\n",
    "from llama_index.core.vector_stores.types import FilterOperator, MetadataFilter\n",
    "\n",
    "with (\n",
    "    GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE']) as graph_store,\n",
    "    VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE']) as vector_store\n",
    "):\n",
    "\n",
    "    query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "        graph_store, \n",
    "        vector_store,\n",
    "        streaming=True,\n",
    "        tenant_id='ecorp',\n",
    "        filter_config = FilterConfig(\n",
    "            MetadataFilter(\n",
    "                key='pub_date',\n",
    "                value='2025-09-01',\n",
    "                operator=FilterOperator.LT\n",
    "            )\n",
    "        ),\n",
    "        no_cache=True\n",
    "    )\n",
    "\n",
    "    response = query_engine.query(\"What are the sales prospects for Example Corp in the UK?\")\n",
    "\n",
    "response.print_response_stream()\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "retrieve_ms: {int(response.metadata['retrieve_ms'])}\n",
    "answer_ms  : {int(response.metadata['answer_ms'])}\n",
    "total_ms   : {int(response.metadata['total_ms'])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1767288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results to be passed to LLM\n",
    "\n",
    "for n in response.source_nodes:\n",
    "    print(n.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
