{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d87f62",
   "metadata": {},
   "source": [
    "# 1 - Indexing (20 mins)\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "In this exercise you will learn how to extract information from unstructured, text-based source documents to build a graph and vector index. \n",
    "\n",
    "In the course of this exercise you will:\n",
    "  \n",
    "  - Inspect the document to be indexed\n",
    "  - Load and extract the data to disk\n",
    "  - Inspect the extracted chunks\n",
    "  - Build the graph and vector index from the extracted data\n",
    "  - Learn about the hierarchical lexical graph model\n",
    "  - Visualise the graph and inferred schema\n",
    "  - Query the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1535a",
   "metadata": {},
   "source": [
    "## Indexing Process\n",
    "\n",
    "The indexing process is split into two pipeline stages:\n",
    "\n",
    "#### 1. Extract\n",
    "\n",
    "Loads and chunks documents, using a Large Language Model (LLM) to perform two extraction steps: proposition extraction, which converts chunked text into well-formed statements, and topic/entity/fact extraction, which identifies relations and concepts.\n",
    "\n",
    "#### 2. Build\n",
    "\n",
    "Populates a graph and creates embeddings from the results of the Extract stage.\n",
    "\n",
    "  \n",
    "![Extract and Build Stages](./images/extract-and-build.png)\n",
    "\n",
    "### Indexing options\n",
    "\n",
    "The GraphRAG Toolkit allows for flexible indexing configurations:\n",
    "\n",
    "  - You can run the Extract and Build pipelines together for continuous ingest.\n",
    "  - You can run the Extract and Build stages separately, for more controlled processing.\n",
    "  - You can configure the tookit to perform batch extraction, which uses Amazon Bedrock batch inference, for handling large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5a1af",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Extract Stage\n",
    "\n",
    "In the following example, you'll run the Extract stage to load some data, chunk the text, and perform both proposition extraction and topic/entity/fact extraction. The results of the Extract stage are then written to disk.\n",
    "\n",
    "![Extract Stage](./images/extract.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee32ed",
   "metadata": {},
   "source": [
    "#### Loading\n",
    "\n",
    "To load data from an external source, you use a LlamaIndex reader. LlamaIndex provides a range of different [readers  and connectors](https://llamaindexxx.readthedocs.io/en/latest/understanding/loading/llamahub.html) for different data sources. In this example, you're going to use a LlamaIndex `SimpleDirectoryReader` to load some Amazon Neptune documentation from a markdown document.\n",
    "\n",
    "#### Chunking\n",
    "\n",
    "Chunking splits a large text into smaller chunks using a [LlamaIndex splitter](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/#text-splitters), such as `MarkdownNodeParser` or `SentenceSplitter`. If you don't explicitly specify a chunking strategy, the GraphRAG Toolkit uses a `SentenceSplitter` by default.\n",
    "\n",
    "#### Extraction\n",
    "\n",
    "The actual extraction consists of two LLM calls per chunk:\n",
    "\n",
    "  1. Extract sets of propositions ‚Äì small, well-formed statements ‚Äì¬†from the chunked text.\n",
    "  2. From the propositions, further extract topics, statements and facts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce06a11",
   "metadata": {},
   "source": [
    "### üîç 1.1 Inspect the document that you are about to index\n",
    "\n",
    "Look at the document that you're about to load by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat source-data/neptune/instance-types.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da2ceb",
   "metadata": {},
   "source": [
    "### üéØ 1.2 Load, chunk and extract data\n",
    "\n",
    "Run the following cell to extract the data and save it to the filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5566f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, IndexingConfig\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory, VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "with (\n",
    "    GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE']) as graph_store,\n",
    "    VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE']) as vector_store\n",
    "):\n",
    "    \n",
    "    # create a LexicalGraphIndex indexing component\n",
    "    \n",
    "    config = IndexingConfig(\n",
    "        chunking=[MarkdownNodeParser()] # chunks document based on markdown headings\n",
    "    )\n",
    "\n",
    "    graph_index = LexicalGraphIndex( # core GraphRAG Toolkit indexing component\n",
    "        graph_store, \n",
    "        vector_store,\n",
    "        indexing_config=config\n",
    "    )\n",
    "    \n",
    "    # load the source document\n",
    "\n",
    "    loader = SimpleDirectoryReader( # reads source documents from filesystem\n",
    "        input_files=[\"./source-data/neptune/instance-types.md\"],\n",
    "        file_metadata=lambda p:{'file_name':os.path.basename(p)}\n",
    "    )\n",
    "    \n",
    "    source_docs = loader.load_data()\n",
    "    \n",
    "    # create a destination for the extracted data\n",
    "    \n",
    "    extracted_docs = FileBasedDocs( # saves extracted data to filesystem\n",
    "        docs_directory='extracted',\n",
    "        collection_id='example-1'\n",
    "    )\n",
    "    \n",
    "    # extract the data\n",
    "    \n",
    "    graph_index.extract(\n",
    "        nodes=source_docs, \n",
    "        handler=extracted_docs,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbe48a",
   "metadata": {},
   "source": [
    "### üîç 1.3 Inspect the extracted data\n",
    "\n",
    "Before you proceed to the Build stage, take at look at the results of the Extract stage. View one of the extracted chunks that has been saved to disk by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat extracted/example-1/aws::e61317db:7893/aws::e61317db:7893:5cbc4df2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ee59a",
   "metadata": {},
   "source": [
    "There's a lot of detail to each extracted chunk, but these are the fields we're interested in:\n",
    "\n",
    "```\n",
    "{\n",
    "    ...\n",
    "    \"metadata\": {\n",
    "        ...\n",
    "        \"aws::graph::propositions\": [\n",
    "            <propositions>\n",
    "        ],\n",
    "        \"aws::graph::topics\": {\n",
    "            \"topics\": [\n",
    "                <topics/statements/facts>\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    ...\n",
    "    \"text\": <chunk text>,    \n",
    "}\n",
    "```\n",
    "\n",
    "Take a moment to review the contents of these fields:\n",
    "\n",
    "#### text\n",
    "\n",
    "Towards the bottom of the file is the raw text of the chunk. In our example, the `MarkdownNodeParser` split the document into multiple chunks based on markdown headings in the document. This is the text of one of those chunks.\n",
    "\n",
    "#### metadata\n",
    "\n",
    "This field contains the results of the proposition extraction and topic/entity/fact extraction steps. \n",
    "  \n",
    "  - Look at the contents of the `aws::graph::propositions` metadata field. Proposition extraction 'cleans up' the content by extracting sets of well-formed, self-contained propositions from the chunked text.\n",
    "  - Compare the propositions in `aws::graph::propositions` with the original chunk text. Notice how each proposition has been framed as a standalone statement, with pronouns replaced by proper names.\n",
    "  - Next, look at the contents of the `aws::graph::topics` metadata field. Here, the extracted propositions are further broken up into statements and facts, grouped by topics. When you run the Build stage later in this exercise, these topics, statements and facts will be inserted into the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533f0af",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Build Stage\n",
    "\n",
    "The Build stage uses the extracted chunks that you looked at above to create the graph and associated vector embeddings:\n",
    "\n",
    "![Build Stage](./images/build.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f976a",
   "metadata": {},
   "source": [
    "### üéØ 1.4 Build the graph and vector stores from the previously extracted chunks\n",
    "\n",
    "Run the cell below to populate the graph and vector stores from the chunks you extracted earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efaf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs\n",
    "\n",
    "with (\n",
    "    GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE']) as graph_store,\n",
    "    VectorStoreFactory.for_vector_store(\n",
    "        os.environ['VECTOR_STORE'], \n",
    "        index_names=['chunk'] # valid values include 'chunk', 'topic', 'statement'\n",
    "    ) as vector_store\n",
    "):\n",
    "    # create a LexicalGraphIndex indexing component \n",
    "    \n",
    "    graph_index = LexicalGraphIndex(\n",
    "        graph_store, \n",
    "        vector_store\n",
    "    )\n",
    "    \n",
    "    # specify the source of previously extracted data\n",
    "    \n",
    "    extracted_docs = FileBasedDocs(\n",
    "        docs_directory='extracted',\n",
    "        collection_id='example-1'\n",
    "    )\n",
    "    \n",
    "    # populate the graph and vector stores\n",
    "\n",
    "    graph_index.build(\n",
    "        nodes=extracted_docs, \n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf153c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Lexical Graph\n",
    "\n",
    "The GraphRAG Toolkit builds a particular kind of graph model, called a hierarchical lexical graph.\n",
    "\n",
    "This graph has a very specific purpose: to make it easy to find sets of relevant statements that can be used to answer the user's question.\n",
    "\n",
    "Statements are the key data elements in the lexical graph. You can think of the entire lexical graph as a \"bucket\" of statements, with other nodes (sources, chunks, topics, facts and entities) playing specific roles to help find and organize statements:\n",
    "\n",
    "  - **Sources** Contain source document metadata for filtering and versioning\n",
    "  - **Chunks** Provide vector-based entry points into the graph\n",
    "  - **Statements** Standalone propositions - the primary unit of context supplied to an LLM\n",
    "  - **Topics** Thematically group statements belonging to the _same_ source\n",
    "  - **Facts** Connect statements belonging to _different_ sources\n",
    "  - **Entities** Represent domain semantics of dataset\n",
    "\n",
    "![Lexical Graph](./images/lexical-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52f491",
   "metadata": {},
   "source": [
    "### üéØ 1.6 Visualise the graph\n",
    "\n",
    "You can visualise the graph that you've just built by running the cells below. Click on the **Graph** tab in the output to view the visualisation. You should see a single source document, several chunks, and then the topics, statements, facts and entities derived from those chunks.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "üîß <b style=\"color:black;\">Jupyter Notebook formatting</b>\n",
    "\n",
    "If you're running in JupyterLab or version 7 of the Jupyter Notebook application, set the `NB_CLASSIC` variable below to `False`. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "‚ö†Ô∏è <b style=\"color:black;\">Display errors</b>\n",
    "\n",
    "Note that sometimes with the Classic Notebook setup, the display can break, and you have to scroll through the output (including what may appear to be JavaScript errors) to get to the **Graph** tab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSIC = True\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.visualisation import GraphNotebookVisualisation\n",
    "\n",
    "v = GraphNotebookVisualisation(nb_classic=NB_CLASSIC)\n",
    "v.display_sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec37ef",
   "metadata": {},
   "source": [
    "### üéØ 1.7 Visualise the inferred schema\n",
    "\n",
    "As well as a lexical graph, the GraphRAG Toolkit also creates schema nodes that represent the implicit domain semantics at the entity-relationship tier (the lowest tier of the lexical graph structure). You can view this schema by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9e682",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NB_CLASSIC = True\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.visualisation import GraphNotebookVisualisation\n",
    "\n",
    "v = GraphNotebookVisualisation(nb_classic=NB_CLASSIC)\n",
    "v.display_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61a393",
   "metadata": {},
   "source": [
    "If you later run the optional notebook, **03 - Agentic Use Cases**, you will see how this ability to infer the schema is used to create descriptions of domain-specific agentic tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396afde9",
   "metadata": {},
   "source": [
    "### üéØ 1.8 Query the Data\n",
    "\n",
    "You can now ask a question of your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "\n",
    "with (\n",
    "    GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE']) as graph_store,\n",
    "    VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE']) as vector_store\n",
    "):\n",
    "\n",
    "    query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "        graph_store, \n",
    "        vector_store,\n",
    "        streaming=True,\n",
    "        no_cache=True\n",
    "    )\n",
    "\n",
    "    response = query_engine.query(\"What are the differences between the r7i and r8g instance families?\")\n",
    "    \n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af467e",
   "metadata": {},
   "source": [
    "## Next Exercise\n",
    "\n",
    "Go to <a href=\"../../../nbclassic/notebooks/graphrag-toolkit/2-Querying.ipynb\"><b>Exercise 2 - Querying</b></a> to continue the workshop exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
