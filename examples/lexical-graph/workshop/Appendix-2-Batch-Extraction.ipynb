{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923f97a8",
   "metadata": {},
   "source": [
    "# Appendix 2 - Batch Extraction\n",
    "\n",
    "In this workshop, you've seen how to index data on a chunk-by-chunk basis – either using separate Extract and Build steps, or combined in a single operation. In both cases, individual chunks are passed to an LLM for extraction.\n",
    "\n",
    "For large datasets, this chunk-by-chunk approach can be slow and costly. [Batch extraction](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/batch-extraction.md) is a feature of the GraphRAG Toolkit that can significantly improve extraction performance for large datasets It is used with [Amazon Bedrock batch inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html) in the Extract stage of the indexing process. Batch extraction submits large batches of chunks to Bedrock for processing simltaneously. Bedrock batch inference workloads are charged at a 50% discount compared to On-Demand pricing.\n",
    "\n",
    "This Appendix summarizes the batch extraction feature. You won't be using it during this this workshop, but it's good to know it exists for larger workloads.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "To set up batch extraction, users need to create an Amazon S3 bucket in the AWS Region where they will be running batch extraction and create a custom service role for batch inference with appropriate permissions. \n",
    "\n",
    "#### Configuration\n",
    "\n",
    "To use batch extraction with the `LexicalGraphIndex`, a `BatchConfig` object must be created and supplied to the `LexicalGraphIndex` as part of the `IndexingConfig`. This `BatchConfig` object manages the configuration settings for Amazon Bedrock batch inference jobs.\n",
    "\n",
    "#### Example\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig, IndexingConfig\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.extract import BatchConfig\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "    \n",
    "def batch_extract_and_load():\n",
    "    \n",
    "    GraphRAGConfig.extraction_batch_size = 1000\n",
    "\n",
    "    batch_config = BatchConfig(\n",
    "        region='us-west-2',\n",
    "        bucket_name='my-bucket',\n",
    "        key_prefix='batch-extract',\n",
    "        role_arn='arn:aws:iam::111111111111:role/my-batch-inference-role',\n",
    "        max_batch_size=40000\n",
    "    )\n",
    "\n",
    "    indexing_config = IndexingConfig(batch_config=batch_config)\n",
    "\n",
    "    with (\n",
    "        GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE']) as graph_store,\n",
    "        VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE']) as vector_store\n",
    "    ):\n",
    "\n",
    "        graph_index = LexicalGraphIndex(\n",
    "            graph_store, \n",
    "            vector_store,\n",
    "            indexing_config=indexing_config\n",
    "        )\n",
    "\n",
    "        reader = SimpleDirectoryReader(input_dir='path/to/directory')\n",
    "        docs = reader.load_data()\n",
    "\n",
    "        graph_index.extract_and_build(docs, show_progress=True)\n",
    "    \n",
    "batch_extract_and_load()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
